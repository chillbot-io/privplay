# Privplay NER Evaluation Report

**Generated:** 2025-12-16 17:19:06  
**Methodology:** SemEval 2013 Task 9 (nervaluate)  
**Dataset:** ai4privacy  
**Samples:** 100  
**Runtime:** 178.0s  

---

## Executive Summary

| Metric | Strict | Exact | Partial | Type |
|--------|--------|-------|---------|------|
| **Precision** | 10.6% | 11.4% | 13.2% | 12.8% |
| **Recall** | 62.9% | 67.7% | 78.2% | 75.8% |
| **F1** | 18.1% | 19.5% | 22.6% | 21.9% |

### Evaluation Mode Definitions

- **Strict**: Exact boundary match AND exact type match (most conservative)
- **Exact**: Exact boundary match, entity type ignored
- **Partial**: Partial boundary overlap (0.5 credit), type must match
- **Type (ent_type)**: Entity type must match, boundaries can partially overlap

---

## Detailed Metrics

### Strict Evaluation (Gold Standard)

| Metric | Value |
|--------|-------|
| Correct | 39 |
| Incorrect | 16 |
| Partial | 0 |
| Missed (FN) | 7 |
| Spurious (FP) | 313 |
| Possible (Total GT) | 62 |
| Actual (Total Pred) | 368 |

### Counts Summary

| Category | Strict | Exact | Partial | Type |
|----------|--------|-------|---------|------|
| Correct | 39 | 42 | 42 | 47 |
| Incorrect | 16 | 13 | 0 | 8 |
| Partial | 0 | 0 | 13 | 0 |
| Missed | 7 | 7 | 7 | 7 |
| Spurious | 313 | 313 | 313 | 313 |

---

## Per-Entity-Type Performance (Strict)

| Entity Type | Precision | Recall | F1 | Support |
|-------------|-----------|--------|-----|---------|
| CONTACT | 0.0% | 0.0% | 0.0% | 0 |
| CRYPTO | 0.0% | 0.0% | 0.0% | 0 |
| DATE | 24.1% | 77.8% | 36.8% | 9 |
| DIGITAL | 0.0% | 0.0% | 0.0% | 0 |
| FINANCIAL | 0.0% | 0.0% | 0.0% | 0 |
| IDNUM | 0.0% | 0.0% | 0.0% | 0 |
| LOCATION | 16.3% | 53.8% | 25.0% | 26 |
| NAME | 16.2% | 66.7% | 26.1% | 27 |
| ORG | 0.0% | 0.0% | 0.0% | 0 |
| VEHICLE | 0.0% | 0.0% | 0.0% | 0 |

---

## Error Analysis

### False Positives by Type

| Type | Count |
|------|-------|
| NAME | 79 |
| LOCATION | 51 |
| CONTACT | 20 |
| DATE | 20 |
| VEHICLE | 18 |
| CRYPTO | 10 |
| IDNUM | 10 |
| DIGITAL | 10 |
| ORG | 5 |
| FINANCIAL | 5 |
| OTHER | 3 |

### False Negatives by Type

| Type | Count |
|------|-------|
| DIGITAL | 8 |
| NAME | 6 |
| FINANCIAL | 6 |
| OTHER | 3 |
| IDNUM | 1 |
| LOCATION | 1 |

### Type Confusion Matrix

Shows cases where entity was detected but assigned wrong type.

| Ground Truth | Predicted As | Count |
|--------------|--------------|-------|
| DIGITAL | VEHICLE | 3 |
| DIGITAL | NAME | 2 |
| OTHER | VEHICLE | 3 |
| OTHER | LOCATION | 2 |
| OTHER | FINANCIAL | 1 |
| FINANCIAL | IDNUM | 1 |
| FINANCIAL | NAME | 1 |
| LOCATION | NAME | 2 |
| LOCATION | CONTACT | 2 |
| LOCATION | VEHICLE | 1 |
| CONTACT | NAME | 1 |
| IDNUM | FINANCIAL | 1 |

---

## Methodology Notes

### SemEval 2013 Task 9

This evaluation follows the methodology defined in:

> Segura-Bedmar, I., Mart√≠nez, P., & Herrero-Zazo, M. (2013). *SemEval-2013 Task 9: Extraction of Drug-Drug Interactions from Biomedical Texts (DDIExtraction 2013)*. In Proceedings of SemEval.

The evaluation considers entity-level (not token-level) matching with four scenarios:
1. **Strict**: Both boundaries and type must match exactly
2. **Exact**: Boundaries must match exactly, type ignored
3. **Partial**: Boundaries can overlap (partial credit), type must match  
4. **Type**: Type must match, boundaries can overlap

### Dataset

**AI4Privacy PII Masking 400K**: A large-scale PII detection benchmark containing
400k+ samples with 54 PII categories, created for privacy-preserving NLP research.

---

*Report generated by Privplay Academic Evaluation Suite*
